import streamlit as st
import pandas as pd
from bs4 import BeautifulSoup as bs
from requests import get
import sqlite3
import os
import plotly.express as px
import base64


# Configuration g√©n√©rale

st.set_page_config(page_title="Dakar Auto App", layout="wide")


# Introduction
st.markdown("""
# Dakar Auto Data App
This app performs webscraping of vehicle listings from Dakar-Auto over multiple pages.  
You can scrape cars, motorcycles, and rental vehicles, visualize them in an interactive dashboard, and download scraped or existing CSV data directly from the app.

**Python libraries used:** `base64`, `pandas`, `streamlit`, `requests`, `bs4`, `plotly`  
**Data source:** Expat-Dakar ‚Äî Dakar-Auto
""")


# Barre lat√©rale / navigation

st.sidebar.title("Navigation")
page = st.sidebar.radio("Aller √† :", ["Scraping", "Dashboard", "Ancien CSV", "√Ä propos", "√âvaluer l'application"])


# Fonction de scraping
def scrape_dakar_auto(url_base, num_pages):
    df_all = []
    for page_num in range(1, num_pages + 1):
        url = f"{url_base}{page_num}"
        res = get(url)
        #if res.status_code != 200:
        #    continue
        soup = bs(res.content, "html.parser")
        containers = soup.find_all("div", class_="listings-cards__list-item mb-md-3 mb-3")
        for cont in containers:
            try:
                title = cont.find("h2", class_="listing-card__header__title mb-md-2 mb-0")
                if not title: continue
                parts = title.a.text.strip().split()
                brand = parts[0]
                year = parts[-1]
                model = " ".join(parts[1:-1])
                attributes = cont.find_all("li", "listing-card__attribute list-inline-item")
                ref = attributes[0].text.split()[-1].strip() if len(attributes)>0 else None
                km = attributes[1].text.replace("km","").strip() if len(attributes)>1 else None
                gearbox = attributes[2].text.strip() if len(attributes)>2 else None
                fuel = attributes[3].text.strip() if len(attributes)>3 else None
                price_raw = cont.find("h3", "listing-card__header__price font-weight-bold text-uppercase mb-0")
                price = price_raw.text.strip().replace("FCFA","").replace(" ","") if price_raw else None
                owner_raw = cont.find("p", "time-author m-0")
                owner = owner_raw.a.text.replace("Par","").strip() if owner_raw and owner_raw.a else None
                addr_raw = cont.find("div", "col-12 entry-zone-address")
                adress = addr_raw.text.strip().replace("\n","") if addr_raw else None
                df_all.append({
                    "brand": brand, "model": model, "year": year, "ref": ref,
                    "km": km, "fuel": fuel, "gearbox": gearbox,
                    "price": pd.to_numeric(price) if price and price.isnumeric() else None,
                    "owner": owner, "adress": adress
                })
            except: pass
    return pd.DataFrame(df_all)

# Page Scraping
if page == "Scraping":
    st.header("Scraping Dakar Auto")
    num_pages = st.number_input("Nombre de pages √† scraper par cat√©gorie", min_value=1, max_value=50, value=1, step=1)

    URLS = {
        "voitures": "https://dakar-auto.com/senegal/voitures-4?&page=",
        "location": "https://dakar-auto.com/senegal/location-de-voitures-19?&page=",
        "motos": "https://dakar-auto.com/senegal/motos-and-scooters-3?&page="
    }

    if st.button("Lancer le Scraping"):
        with st.spinner("Scraping en cours..."):
            for cat, base_url in URLS.items():
                df_tmp = scrape_dakar_auto(base_url, num_pages)
                df_tmp["category"] = cat
                st.session_state[f"df_{cat}"] = df_tmp
        st.success("Scraping termin√© !")

    
    # Boutons pour chaque cat√©gorie
    for cat in URLS.keys():
        st.subheader(f"Donn√©es {cat.capitalize()}")
        if st.button(f"Afficher {cat}"):
            if f"df_{cat}" in st.session_state:
                st.dataframe(st.session_state[f"df_{cat}"])
            else:
                st.warning(f"Aucune donn√©e disponible pour {cat}. Lancez le scraping d'abord.")

        if f"df_{cat}" in st.session_state:
            csv_bytes = st.session_state[f"df_{cat}"].to_csv(index=False).encode("utf-8")
            st.download_button(
                label=f"T√©l√©charger {cat}",
                data=csv_bytes,
                file_name=f"{cat}_scraped.csv",
                mime="text/csv"
            )

    # Sauvegarde dans SQLite
    conn = sqlite3.connect("dakar_auto_data.db")
    for cat in URLS.keys():
        if f"df_{cat}" in st.session_state:
            st.session_state[f"df_{cat}"].to_sql(cat, conn, if_exists="replace", index=False)
    conn.close()
    st.info("Les donn√©es ont √©t√© sauvegard√©es dans la base SQLite 'dakar_auto_data.db'.")


# Page Dashboard
if page == "Dashboard":
    st.header("Dashboard complet")
    conn = sqlite3.connect("dakar_auto_data.db")
    dfs = {}
    for cat in ["voitures","location","motos"]:
        try:
            dfs[cat] = pd.read_sql(f"SELECT * FROM {cat}", conn)
        except:
            dfs[cat] = pd.DataFrame()
    conn.close()

    if all(df.empty for df in dfs.values()):
        st.info("Aucune donn√©e disponible. Lancez d'abord le scraping.")
    else:
        df_total = pd.concat(dfs.values(), ignore_index=True)

        st.subheader("Informations sur le dataset")
        st.markdown(f"- Dimensions : {df_total.shape[0]} lignes, {df_total.shape[1]} colonnes")
        st.markdown("**Valeurs manquantes par colonne :**")
        st.dataframe(df_total.isnull().sum())
        st.markdown("**Statistiques descriptives :**")
        st.dataframe(df_total.describe(include='all').T)
        st.subheader("Donn√©es brutes")
        st.dataframe(df_total)

        # Visualisations
        if df_total["price"].notnull().any():
            st.plotly_chart(px.histogram(df_total, x="price", nbins=30, title="Distribution des prix"), use_container_width=True)
        
        df_cat_count = df_total["category"].value_counts().reset_index()
        df_cat_count.columns = ["category","category_count"]
        st.plotly_chart(px.bar(df_cat_count, x="category", y="category_count", title="Nombre de listings par cat√©gorie"), use_container_width=True)
        st.plotly_chart(px.pie(df_cat_count, names="category", values="category_count", title="Proportion par cat√©gorie"), use_container_width=True)

        df_brand_count = df_total["brand"].value_counts().reset_index()
        df_brand_count.columns = ["brand","count"]
        st.plotly_chart(px.bar(df_brand_count, x="brand", y="count", title="Nombre de v√©hicules par marque"), use_container_width=True)

        df_top10_brand = df_brand_count.head(10)
        st.plotly_chart(px.pie(df_top10_brand, names="brand", values="count", title="Top 10 marques"), use_container_width=True)

        df_brand_price = df_total.groupby("brand")["price"].mean().reset_index().sort_values("price", ascending=False)
        st.plotly_chart(px.bar(df_brand_price, x="brand", y="price", title="Prix moyen par marque"), use_container_width=True)

        st.plotly_chart(px.box(df_total, x="category", y="price", title="Boxplot : Prix par cat√©gorie"), use_container_width=True)
        top10_brands = df_total["brand"].value_counts().head(10).index
        st.plotly_chart(px.box(df_total[df_total["brand"].isin(top10_brands)], x="brand", y="price", title="Boxplot : Prix par marque (Top 10)"), use_container_width=True)

        df_scatter = df_total[df_total["price"].notnull() & df_total["km"].notnull()]
        if not df_scatter.empty:
            df_scatter["km"] = pd.to_numeric(df_scatter["km"], errors='coerce')
            df_scatter = df_scatter.dropna(subset=["km"])
            st.plotly_chart(px.scatter(df_scatter, x="km", y="price", color="category",
                                       hover_data=["brand","model","year"], title="Prix vs kilom√©trage"), use_container_width=True)

        df_year = df_total[df_total["year"].notnull()]
        st.plotly_chart(px.histogram(df_year, x="year", nbins=20, title="R√©partition des ann√©es des v√©hicules"), use_container_width=True)

        csv = df_total.to_csv(index=False).encode("utf-8")
        st.download_button(label="T√©l√©charger toutes les donn√©es du scraping", data=csv, file_name="dakar_auto_scraped_all.csv", mime="text/csv")


# Page anciens CSV
if page == "Ancien CSV":
    st.header("üìÇ Fichiers CSV existants (data1)")
    data_folder = "data1"
    if os.path.exists(data_folder):
        csv_files = [f for f in os.listdir(data_folder) if f.endswith(".csv")]
        if csv_files:
            for file in csv_files:
                if st.button(f"Afficher {file}"):
                    df_file = pd.read_csv(os.path.join(data_folder, file))
                    st.dataframe(df_file)
                csv_bytes = pd.read_csv(os.path.join(data_folder, file)).to_csv(index=False).encode("utf-8")
                st.download_button(label=f"üì• T√©l√©charger {file}", data=csv_bytes, file_name=file, mime="text/csv")
        else:
            st.warning("Aucun fichier CSV trouv√© dans data1.")
    else:
        st.info("Le dossier data1 n'existe pas.")

# Page √Ä propos
if page == "√Ä propos":
    st.header("√Ä propos de l'application")
    st.markdown("""
    Cette application est inspir√©e de **MyBestApp-2025**.
    Elle permet de scraper les annonces de Dakar Auto, visualiser les donn√©es,
    t√©l√©charger les CSV, et explorer les anciens fichiers existants.
    """)
    st.markdown("Cr√©√©e par : *OGOUNCHI G√©raud*")


# Page √âvaluer l'application
if page == "√âvaluer l'application":
    st.header("√âvaluez mon application")
    st.markdown("Merci de prendre un moment pour √©valuer cette application en utilisant le formulaire ci-dessous :")
    st.markdown(
        """<iframe src="https://ee.kobotoolbox.org/x/2LOA6Lk0" width="100%" height="800" frameborder="0"></iframe>""",
        unsafe_allow_html=True
    )

